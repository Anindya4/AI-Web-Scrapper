# AI Web Scraper ğŸ¤–ğŸŒ

AI Web Scraper is a powerful and intuitive web scraping application that leverages Large Language Models (LLMs) to extract specific information from websites based on natural language commands. Instead of writing complex selectors, you can simply tell ğŸ—£ï¸ the AI what you want, and it will parse the data for you âœ¨.  
The application is built with a user-friendly interface using Streamlit, making it easy for anyone to scrape, view, and parse web content. ğŸ“ŠğŸ’»

# Key Features ğŸš€
* **Simple URL-based Scraping:** Enter any website URL to fetch its full DOM content. ğŸ”—
* **Intelligent AI Parsing:** Uses Ollama with Llama 3.1 via LangChain to understand your requests and extract the exact information you need. ğŸ§ ğŸ”
* **Natural Language Queries:** No need for CSS selectors or XPath. Just ask for the data you want (e.g., "List all the article titles" or "Extract the product names and prices"). ğŸ’¬ğŸ’¡
* **Interactive UI:** A clean and simple web interface built with Streamlit allows you to see the scraped content and interact with the AI parser easily. ğŸ–¥ï¸âœ¨
* **Content Cleaning:** Automatically removes unnecessary scripts and styles from the scraped HTML to provide clean text to the AI for better accuracy. ğŸ§¹ğŸ“„

# Technologies Used ğŸ› ï¸
### This project is built with a modern stack of Python ğŸ libraries for web scraping, AI integration, and web development.
* **Backend & Core:**
  * Python
* **AI & LLM Integration:**
  * LangChain: Framework for developing applications powered by language models ğŸ“–.
  * Ollama (`langchain_ollama`): For running the Llama 3.1 model locally. ğŸ§ 
* **Web Scraping:**
  * Selenium: For browser automation and scraping dynamic JavaScript-heavy websites ğŸ•¸ï¸.
  * BeautifulSoup4: For parsing HTML and extracting/cleaning content ğŸ².
* **Web Framework:**
  * Streamlit: For creating and running the interactive web application UI ğŸ“Š.
* **Dependencies:**
  * lxml, html5lib: Efficient parsers for BeautifulSoup.
  * `python-dotenv`: For managing environment variables ğŸ”‘.

# Setup and Installation âš™ï¸
To run this project locally, you will need to have Python ğŸ and Ollama ğŸ¦™ installed.

## **1. Prerequisites: Install Dependencies ğŸ“¥**

Download the given `req.txt` file on you machine and run:  
```sh
pip install -r req.txt
```
***Note: The `scrape.py` file is currently configured to use a remote Selenium WebDriver service (Bright Data). To run this locally without that service, you will need to modify scrape.py to use a local chromedriver ğŸ”§.***

It'll install all the required libraries on your machine.  
You might as well crate a virtual environment on your machine before installing the dependencies by:

```python
python -m venv {your_venv_name}
``` 
## **2. Clone the Repository ğŸ“‚**
Clone this repository to your local machine:

```bash
git clone https://github.com/Anindya4/AI-Web-Scrapper.git
cd AI-Web-Scrapper
```

















